---
marp: true
theme: default
paginate: true
# _class: invert
# color: white
# class: lead
# section.lead h1 {
#   text-align: center;
# }
style: |
  section.lead h1 {
    text-align: center;
  },
  section.lead h2 {
    text-align: center;
  },
  section.lead h3 {
    text-align: center;
  },
  h1 {
    color: #3d3d3d;
  },
  h2 {
    color: #3d3d3d;
  },
  h3 {
    color: #3d3d3d;
  }
# style: |
#   section {
#     background-color: #ffffff;
#   }
#   h1 {
#     font-size: 50px;
#     color: #2a2a2a;
#   }
---

<!-- _class: lead -->

# Uncertainty-Based Offline Reinforcement Learning with Diversified Q-Ensemble

## ICML'21

## Citation Count: 1

### Gaon An, Seungyong Moon, Jang-Hyun Kim, Hyun Oh Song

### Seoul National University, Neural Processing Research Center, DeepMetrics

---

# Offline RL

![width:1000px](img/offline_rl_small.png)

We aim to learn a policy $\pi$ from the history of trajectories $\mathcal{D} = \{ s_t, a_t, s_t', r_t \}$ generated by behavioral policy $\pi_{\beta}$ s.t. the performance $\pi \geq \pi_{\beta}$(Which means we want to train an agent's policy $\pi$ only from the history of trajectories $\mathcal{D}$)

![](img/offline_rl_algo.png)

---

# Challenge 

- Extrapolation Error
  
  - Because the agent's policy isn't the same as the behavioral policy. The agent may overestimate unseen $Q^{\pi}(s, a)$, which gives a higher $Q^{\pi}(s, a)$ value than the ground-truth $Q^{\pi}(s, a)$ value .
  - In the offline setting, the policy cannot correct such over-optimistic Q-values.
  
- Solution

    - If we add a regularizer to the equation in order to make the agent (1) underestimate the $Q^{\pi}(s, a)$ value of unseen state-action pair $(s, a)$ or (2) choose the state-action pair that closes to the state-action pair already in the history trajectory $\mathcal{D}$, we can avoid the crazy actions that the agent may do.
    - But a trade-off between Optimality and Conservativeness.

---

# Idea

