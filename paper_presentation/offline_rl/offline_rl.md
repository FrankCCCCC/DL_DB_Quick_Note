---
marp: true
theme: default
paginate: true
# _class: invert
# color: white
# class: lead
# section.lead h1 {
#   text-align: center;
# }
style: |
  section.lead h1 {
    text-align: center;
  },
  section.lead h2 {
    text-align: center;
  },
  section.lead h3 {
    text-align: center;
  },
  h1 {
    color: #3d3d3d;
  },
  h2 {
    color: #3d3d3d;
  },
  h3 {
    color: #3d3d3d;
  }
# style: |
#   section {
#     background-color: #ffffff;
#   }
#   h1 {
#     font-size: 50px;
#     color: #2a2a2a;
#   }
---

<!-- _class: lead -->

# Uncertainty-Based Offline Reinforcement Learning with Diversified Q-Ensemble

## ICML'21

## Citation Count: 1

### Gaon An, Seungyong Moon, Jang-Hyun Kim, Hyun Oh Song

### Seoul National University, Neural Processing Research Center, DeepMetrics

---

# Offline RL

![width:1000px](img/offline_rl_small.png)

We aim to learn a policy $\pi$ from the history of trajectories $\mathcal{D} = \{ s_t, a_t, s_t', r_t \}$ generated by behavioral policy $\pi_{\beta}$ s.t. the performance $\pi \geq \pi_{\beta}$(Which means we want to train an agent's policy $\pi$ only from the history of trajectories $\mathcal{D}$)

![](img/offline_rl_algo.png)

---

# Challenge 

- Extrapolation Error
  
  - Because the agent's policy isn't the same as the behavioral policy. The agent may overestimate unseen $Q^{\pi}(s, a)$, which gives a higher $Q^{\pi}(s, a)$ value than the ground-truth $Q^{\pi}(s, a)$ value .
  - In the offline setting, the policy cannot correct such over-optimistic Q-values.
  
- Solution

    - If we add a regularizer to the equation in order to make the agent (1) underestimate the $Q^{\pi}(s, a)$ value of unseen state-action pair $(s, a)$ or (2) choose the state-action pair that closes to the state-action pair already in the history trajectory $\mathcal{D}$, we can avoid the crazy actions that the agent may do.
    - But a trade-off between Optimality and Conservativeness.

---

# Idea

- Penalize the Q-function with the most pessimistic Q-network of the ensemble Q-network.

Modify the following SAC objective

$$
\min_{\phi_{i}} \mathbb{E}_{\left(\mathbf{s}, \mathbf{a}, \mathbf{s}^{\prime}\right) \sim \mathcal{D}}\left[\left(Q_{\phi}(\mathbf{s}, \mathbf{a})-\left(r(\mathbf{s}, \mathbf{a})+\gamma \mathbb{E}_{\mathbf{a}^{\prime} \sim \pi_{\theta}\left(\cdot \mid \mathbf{s}^{\prime}\right)}\left[Q_{\phi^{\prime}}\left(\mathbf{s}^{\prime}, \mathbf{a}^{\prime}\right)\right]  - \beta \log \pi_{\theta}\left(\mathbf{a}^{\prime} \mid \mathbf{s}^{\prime}\right) \right)\right)^{2}\right]
$$

$$
\max_{\theta} \mathbb{E}_{\mathbf{s} \sim \mathcal{D}, \mathbf{a} \sim \pi_{\theta}(\cdot \mid \mathbf{s})}\left[Q_{\phi}(\mathbf{s}, \mathbf{a})-\beta \log \pi_{\theta}(\mathbf{a} \mid \mathbf{s})\right]
$$

<!-- ![](./img/q_network_objective.png) -->

To SAC-N

$$
\min_{\phi_{i}} \mathbb{E}_{s, a, s^{\prime} \sim \mathcal{D}}\left[\left(Q_{\phi_{i}}(s, a)-\left(r(s, a)+\gamma \mathbb{E}_{a^{\prime} \sim \pi_{\theta}\left(\cdot \mid s^{\prime}\right)}\left[\min _{j=1, \ldots, N} Q_{\phi_{j}^{\prime}}\left(s^{\prime}, a^{\prime}\right) - \beta \log \pi_{\theta}\left(a^{\prime} \mid s^{\prime}\right)\right]\right)\right)^{2}\right] \\
$$

$$
\max_{\theta} \mathbb{E}_{s \sim \mathcal{D}, a \sim \pi_{\theta}(\cdot \mid s)}\left[\min _{j=1, \ldots, N} Q_{\phi_{j}}(s, a)-\beta \log \pi_{\theta}(a \mid s)\right]
$$

<!-- ![](./img/SAC-N.png) -->

---

# Idea

And the authors surprisingly found that SAC-N will outperform than the SOTA offline-RL algorithm CQL when the number of ensemble is large enough.

![](./img/SAC-N_vs_CQL.png)

---

# Idea

- Obviously, the redundant Q-networks of SAC-N cost lots of computation. The authors aim to reduce the size of the ensemble Q-network while achieving the same performance.
- The authors found that the performance of SAC-N is negatively correlated with the degree to which the input gradients of Q-functions $\nabla_a Q_{\phi_j} (s, a)$ are aligned, which increases with $N$.

---

# Evidence 1

**The Q-value predictions for the OOD actions have a higher variance.**

![](./img/SAC-N_OOD_props.png)

---

- Here we define the penalty from the clipping as 

$$
\mathbb{E}_{s \sim D,a \sim \pi(·|s)} \left[ \frac{1}{N} \sum_{j=1}^{N}Q_{\phi_j}(s, a) − \min_{j=1,...,N} Q_{\phi_j}(s, a) \right]
$$

- We also approximate the expected minimum of the realizations following the work of Royston

$$
\mathbb{E} \left[ \min_{j=1,...,N} Q_j(s, a) \right] \approx m(s, a) − \Phi^{−1} \left( \frac{N − \frac{\pi}{8}}{N − \frac{\pi}{4} + 1} \right) \sigma(s, a)
$$

Where we suppose $Q(s, a) \sim \mathcal{N}(m(s, a), \sigma(s, a))$ and $\Phi$ is the CDF of the standard Gaussian distribution.

---

# Evidence 2

**The performance of the learned policy degrades significantly when the Q-functions share a similar local structure.**

![width:500px](./img/cosine_similarity_avg_reward.png)

---

The minimum cosine similarity between the gradients of the Q-functions is

$$
\min_{i \neq j} \langle \nabla_a Q_{\phi_i} (s, a), \nabla_a Q_{\phi_j}(s, a) \rangle
$$

![width:800px](./img/diversification.png)