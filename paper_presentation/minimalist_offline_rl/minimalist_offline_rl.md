---
marp: true
theme: default
paginate: true
# _class: invert
# color: white
class: lead
# section.lead h1 {
#   text-align: center;
# }
style: |
  section.lead h1 {
    text-align: center;
  },
  section.lead h2 {
    text-align: center;
  },
  section.lead h3 {
    text-align: center;
  },
  h1 {
    color: #3d3d3d;
  },
  h2 {
    color: #3d3d3d;
  },
  h3 {
    color: #3d3d3d;
  }
# style: |
#   section {
#     background-color: #ffffff;
#   }
#   h1 {
#     font-size: 50px;
#     color: #2a2a2a;
#   }
---

# A Minimalist Approach to Offline Reinforcement Learning

### NIP'21 Spotlight Citation: 42

### Scott Fujimoto, Shixiang Shane Gu

### Mila, McGill University, Google Research, Brain Team

---

<style>
img[alt~="center"] {
  display: block;
  margin: 0 auto;
}
</style>

# Motivation

![bg right cover](img/self_driving_car.jpg)

- It's impossible to let RL learn from real environment due to safety consideration.

- The simulator is always different from the real world and it's costly.

- Can we train an agent only from offline dataset without interacting with the environment/simulator? 

---
<style>
img[alt~="center"] {
  display: block;
  margin: 0 auto;
}
</style>

# Offline RL

![center width:1000px](img/offline_rl_small.png)

We aim to learn a policy $\pi$ from the history of trajectories $\mathcal{D} = \{ s_t, a_t, s_t', r_t \}$ generated by behavioral policy $\pi_{\beta}$ s.t. the performance $\pi \geq \pi_{\beta}$(Which means we want to train an agent's policy $\pi$ only from the history of trajectories $\mathcal{D}$)

---
<style>
img[alt~="center"] {
  display: block;
  margin: 0 auto;
}
</style>

# Offline RL Challenge

- The dataset doesn't cover everything in the environment
- The agent that collects the dataset isn't optimal/expert

Offline RL conquers these issues but yields other issues

- Additional computation cost
- Difficult to implement(includes many minor but matter code-;level improvement)

---
<style>
img[alt~="center"] {
  display: block;
  margin: 0 auto;
}
</style>

# Problem Formulation

What's the minimalist adjustment to build an offline RL algorithm?

---

# Idea

Actually, we only need to add a **behavior cloning** to regularize the 