---
marp: true
theme: default
paginate: true
# _class: invert
# color: white
class: lead
# section.lead h1 {
#   text-align: center;
# }
style: |
  section.lead h1 {
    text-align: center;
  },
  section.lead h2 {
    text-align: center;
  },
  section.lead h3 {
    text-align: center;
  },
  h1 {
    color: #3d3d3d;
  },
  h2 {
    color: #3d3d3d;
  },
  h3 {
    color: #3d3d3d;
  }
# style: |
#   section {
#     background-color: #ffffff;
#   }
#   h1 {
#     font-size: 50px;
#     color: #2a2a2a;
#   }
---

# A Minimalist Approach to Offline Reinforcement Learning

### NIP'21 Spotlight Citation: 42

### Scott Fujimoto, Shixiang Shane Gu

### Mila, McGill University, Google Research, Brain Team

---

<style>
img[alt~="center"] {
  display: block;
  margin: 0 auto;
}
</style>

# Motivation

![bg right cover](img/self_driving_car.jpg)

- It's impossible to let RL learn from real environment due to safety consideration.

- The simulator is always different from the real world and it's costly.

- Can we train an agent only from offline dataset without interacting with the environment/simulator? 

---
<style>
img[alt~="center"] {
  display: block;
  margin: 0 auto;
}
</style>

# Offline RL

![center width:1000px](img/offline_rl_small.png)

We aim to learn a policy $\pi$ from the history of trajectories $\mathcal{D} = \{ s_t, a_t, s_t', r_t \}$ generated by behavioral policy $\pi_{\beta}$ s.t. the performance $\pi \geq \pi_{\beta}$(Which means we want to train an agent's policy $\pi$ only from the history of trajectories $\mathcal{D}$)

On the other hand, the imitation learning only mimic the expert policy without reward.

---
<style>
img[alt~="center"] {
  display: block;
  margin: 0 auto;
}
</style>

# Offline RL Challenge

- The dataset doesn't cover everything in the environment
- The agent will go crazy(improper Q value) in the unseen state-action.

---
<style>
img[alt~="center"] {
  display: block;
  margin: 0 auto;
}
</style>

# Solution 

- Let the agent only take the best action that appears in the dataset or close to

Offline RL conquers these issues but yields other issues

- Additional computation cost
- Difficult to implement(includes many minor but matter code-level improvement)

---

# Issues of Offline RL 1: Additional computation cost

- SAC-N
  
  - Take the actions that have high Q-value surely, which means avoid action that has high variance Q-value or low Q-value
  - Train $N$ Q-network and update them with the gradient of the most pessimistic network, which is the network gives lowest Q-value.

---

- EDAC
  
  - But it need hundreds of network. Can we use less network to evaluate the the most pessimistic network.
  - According to the Royston equation, if we enlarge the variance of the $N$ Q-network, we can evaluate the most pessimistic network with less Q-network.
  - Idea: Minimize the similarity of the gradient $\nabla_{a}Q_{\phi_i}(s, a)$ between $N$ Q-networks $Q_{\phi_i}$.
  - But it still needs tens of Q-networks.

---
<style>
img[alt~="center"] {
  display: block;
  margin: 0 auto;
}
</style>

![center width:1000px](img/diversification.png)

---
<style>
img[alt~="center"] {
  display: block;
  margin: 0 auto;
}
</style>

# Issues of Offline RL 2: Difficult to implement

![center width:1000px](img/impl_comp.png)

---
<style>
img[alt~="center"] {
  display: block;
  margin: 0 auto;
}
</style>

![center width:1000px](img/impl_comp_exp_perc.png)

Performance difference between with and without implementation changes

---
<style>
img[alt~="center"] {
  display: block;
  margin: 0 auto;
}
</style>

# Problem Formulation

### What's the minimalist adjustment to build an offline RL algorithm?

---

# Idea

### Actually, we only need to add a *behavior cloning* to regularize the 

---

Note:

1. High-level Fisher-BRC?
2. What's extrapolation error?
3. What's the max over sampled actions?