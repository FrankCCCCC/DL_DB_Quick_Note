# Neural Tangent Kernel

### Wide Neural Networks of Any Depth Evolve as Linear Models Under Gradient Descent

[PDF Highlight](./Wide%20Neural%20Networks%20of%20Any%20Depth%20Evolve%20as.pdf)

### Every Model Learned by Gradient Descent Is Approximately a Kernel Machine

University of Washington

[Paper Explained Video](https://www.youtube.com/watch?v=ahRPdiCop3E)

### Finding sparse trainable neural networks through Neural Tangent Transfer

[Blog](https://zenkelab.org/2020/06/paper-finding-sparse-trainable-neural-networks-through-neural-tangent-transfer/)

[Paper](https://arxiv.org/abs/2006.08228)

### Deep Neural Networks as Gaussian Processes

ICLR 2018

Citation 370

[Paper](https://arxiv.org/abs/1711.00165)

[PDF Highlight](./DEEP%20NEURAL%20NETWORKS%20AS%20GAUSSIAN%20PROCESSES.pdf)

Propose NNGP.

### Generalization bounds of stochastic gradient descent for wide and deep neural networks

ICML 2019

Citation 129

[Paper](http://proceedings.mlr.press/v97/allen-zhu19a.html)

[PDF Highlight](./stochastic_gradient/Generalization%20bounds%20of%20stochastic%20gradient%20descent%20for%20wide%20and%20deep%20neural%20networks..pdf)

Propose **Neural Tangent Random Feature (NTRF)**.


### A Convergence Theory for Deep Learning via Over-Parameterization

ICML 2019

Citation 619

[Paper](http://proceedings.mlr.press/v97/allen-zhu19a.html)

[PDF Highlight](./A%20Convergence%20Theory%20for%20Deep%20Learning%20via%20Over-Parameterization.pdf)



### Learning and Generalization in Overparameterized Neural Networks, Going Beyond Two Layers

NIPS 2019

Citation 407

[Paper](https://proceedings.neurips.cc/paper/2019/hash/62dad6e273d32235ae02b7d321578ee8-Abstract.html)

[PDF Highlight](./Learning%20and%20Generalization%20in%20Overparameterized%20Neural%20Networks,%20Going%20Beyond%20Two%20Layers.pdf)

### On Exact Computation with an Infinitely WideNeural Net

[Paper](https://arxiv.org/abs/1904.11955)

[PDF Highlight](./On%20Exact%20Computation%20with%20an%20Infinitely%20Wide%20Neural%20Net.pdf)

Citation 344

Propose **Convolutional Neural Tangent Kernel(CNTK)**.

### Neural Tangent Kernel:Convergence and Generalization in Neural Networks

[Paper](https://arxiv.org/abs/1806.07572)

[PDF Highlight](./Neural%20Tangent%20Kernel%20Convergence%20and%20Generalization%20in%20Neural%20Networks.pdf)

Propose **Neural Tangent Kernel(NTK)**.

### Neural Contextual Bandits with UCB-based Exploration

It propose a way to use NTK features for UCB exploration/exploitation.

### What Can ResNet Learn Efficiently, Going Beyond Kernels?

NIPS 2019

Citation 75

[Full Paper(Version 3)](https://arxiv.org/abs/1905.10337)

[NIPS Version](https://papers.nips.cc/paper/2019/hash/5857d68cd9280bc98d079fa912fd6740-Abstract.html)

The paper shows that NTK might not represent all learning capacity of the DNN.

---

## Reference
- [深度学习理论之Neural Tangent Kernel第一讲：介绍和文献总结](https://zhuanlan.zhihu.com/p/105871604)
- [Understanding the Neural Tangent Kernel By Rajat's Blog](https://rajatvd.github.io/NTK/)
  - Code for the blog [rajatvd/NTK](https://github.com/rajatvd/NTK)
- [Ultra-Wide Deep Nets and the Neural Tangent Kernel (NTK)](https://blog.ml.cmu.edu/2019/10/03/ultra-wide-deep-nets-and-the-neural-tangent-kernel-ntk/)
- [A Visual Exploration of Gaussian Processes](https://distill.pub/2019/visual-exploration-gaussian-processes/#GaussianProcesses)
- [thegregyang/NTK4A](https://github.com/thegregyang/NTK4A)
- [Some Intuition on the Neural Tangent Kernel](https://www.inference.vc/neural-tangent-kernels-some-intuition-for-kernel-gradient-descent/)
- [NTK推导的详细过程](https://zhuanlan.zhihu.com/p/166158072)
- [CMU ML Blog: Ultra-Wide Deep Nets and the Neural Tangent Kernel (NTK)](https://blog.ml.cmu.edu/2019/10/03/ultra-wide-deep-nets-and-the-neural-tangent-kernel-ntk/)
- [Some Intuition on the Neural Tangent Kernel](https://www.inference.vc/neural-tangent-kernels-some-intuition-for-kernel-gradient-descent/)
- [直观理解Neural Tangent Kernel](https://zhuanlan.zhihu.com/p/339971642)
- [Deep Gaussian Processes](http://inverseprobability.com/talks/notes/deep-gaussian-processes.html)
- [知乎 -【强化学习 139】PreQN](https://zhuanlan.zhihu.com/p/162496935)
- [Effortless optimization through gradient flows](https://francisbach.com/gradient-flows/)
- [Chang Liu - Gradient Flow](http://ml.cs.tsinghua.edu.cn/~changliu/static/Gradient-Flow.pdf)
- [When are Neural Networks more powerful than Neural Tangent Kernels?](https://blog.einstein.ai/beyond-ntk/)
- [Neural networks are fundamentally Bayesian](https://towardsdatascience.com/neural-networks-are-fundamentally-bayesian-bee9a172fad8)
- [知乎 - Neural Contextual Bandits with UCB-based Explor...](https://zhuanlan.zhihu.com/p/262608477)