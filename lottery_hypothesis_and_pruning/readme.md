# Lottery Hypothesis & Model Compression

## Can we get a better and compact model?

---

## Proving the Lottery Ticket Hypothesis Pruning is All You Need

[PDF Highlight](./Proving_the_Lottery_Ticket_Hypothesis_Pruning_is_All_You_Need.pdf)
  
Try to prove Lottery Hypothesis with math

---

## The Lottery Ticket Hypothesis Finding Sparse Trainable Neural Networks

[PDF Highlight](./THE_LOTTERY_TICKET_HYPOTHESIS_FINDING_SPARSE_TRAINABLE_NEURAL_NETWORKS.pdf)

### Introduction

æå‡ºLottery Hypothesisï¼Œåœ¨ä¸€å€‹ç¥ç¶“ç¶²è·¯ä¸­ï¼Œå­˜åœ¨ä¸€å€‹è¼ƒå°çš„ç¶²è·¯(called Lottery Ticket or Winning Ticket, 10-20% or less of the original network size)ï¼Œä¸”ä»¥åŸç¶²è·¯çš„æ–¹å¼Initializeå¾Œä¸¦Retrainå¯ä»¥å¾—åˆ°æ¯”åŸç¶²è·¯åŒç­‰æˆ–æ˜¯æ›´å¥½çš„Test Accuracyã€‚ç„¶è€Œï¼Œè‹¥ç”¨Random Initializeçš„ä¸¦è¨“ç·´ï¼Œå‰‡å­¸ç¿’é€Ÿåº¦å’ŒTest Accuracyéƒ½æœƒä¸‹é™ã€‚

### Theorem: Lottery Hypothesis

The lottery ticket hypothesis predicts that âˆƒ m for which j
0 â‰¤ j (commensurate training time), a 0 â‰¥ a (commensurate accuracy), and kmk0  |Î¸| (fewer parameters)

çµ¦å®šä»»ä¸€ç¥ç¶“ç¶²è·¯Î¸ï¼ŒExist ä¸€å€‹ç¥ç¶“ç¶²è·¯Î¸'ï¼ŒåŠMask m = {0, 1}ï¼Œä½¿ = m * Î¸ï¼ŒÎ¸ >> Î¸'ï¼Œåˆ†åˆ¥è¨“ç·´ç¥ç¶“ç¶²è·¯Î¸' j' iterationã€Î¸ j iterationï¼Œåœ¨åŒé‡çš„(commensurate)ä¸”è¨“ç·´é‡ä¸‹j' <= jï¼Œç¥ç¶“ç¶²è·¯Î¸, Î¸'å¯ä»¥é”åˆ°Test Accuracy a, a'ï¼Œä¸”a' >= a

> commensurate: åŒé‡çš„ï¼Œç›¸ç¨±çš„

### Identifying winning tickets

**Steps:**

1. Randomly initialize a neural network f(x; Î¸_0) (where Î¸_0 âˆ¼ D_Î¸).
   
2. Train the network for j iterations, arriving at parameters Î¸_j .
   
3. Prune p% of the parameters in Î¸_j , creating a mask m.
   
4. Reset the remaining parameters to their values in Î¸0, creating the winning ticket f(x; m * Î¸_0).

> æ¯æ¬¡Trainç¥ç¶“ç¶²è·¯jå€‹iterationå¾Œï¼Œprune p%çš„ç¥ç¶“å…ƒï¼Œå¦‚æ­¤åè¦†è¨“ç·´ã€ä¿®å‰ª(Iterative Pruning)for n roundï¼Œå°±å¯ä»¥å¾—åˆ°Lottery Tickets

### Winning Tickets in Fully-Connected Networks

**Iterative Pruning**

### Winning Tickets in Convolutional Networks

### VGG and ResNet for CIFAR10

**High Learning Rate Failure**
At the higher learning rate, iterative pruning does not find winning tickets, and performance is no better than when the pruned networks are randomly reinitialized. However, at the lower learning rate, the usual pattern reemerges, with subnetworks that remain within 1 percentage point of the original accuracy while Pm â‰¥ 3.5%.

To bridge the gap between the lottery ticket behavior of the lower learning rate and the accuracy advantage of the higher learning rate, we explore the effect of linear learning rate warmup from 0 to the initial learning rate over k iterations. Training VGG-19 with warmup (k = 10000, green line) at learning rate 0.1 improves the test accuracy of the unpruned network by about one percentage point.

![High LR Failure](imgs/lottery/fig7.png)

### appendix E

1. Networks found via iterative pruning with the original initializations (blue in Figure 14).
   
2. Networks found via iterative pruning that are randomly reinitialized (orange in Figure 14).
   
3. Random sparse subnetworks with the same number of parameters as those found via iterative pruning (green in Figure 14).

we find that the randomly reinitialized networks outperform random sparsity. However, for all of the other, convolutional networks studied in this paper, there is no significant difference in performance between the two. We hypothesize that the fully-connected
network for MNIST sees these benefits because only certain parts of the MNIST images contain useful information for classification, meaning connections in some parts of the network will be more valuable than others.

![fig14](imgs/lottery/fig14.png)

### Appendix F.3

Figure 17 shows the performance of winning tickets whose initializations
are randomly sampled from the distribution of initializations contained in the winning tickets for adam. More concretely, let Dm = {Î¸(i)0|m(i) = 1} be the set of initializations found in the winning
ticket with mask m. We sample a new set of parameters Î¸00 âˆ¼ Dm and train the network f(x; mÎ¸00). We perform this sampling on a per-layer basis. The results of this experiment are in Figure 17. Winning tickets reinitialized from Dm perform little better than when randomly reinitialized from D

![fig17](imgs/lottery/fig17.png)

### Appendix G.2

According to the graph on the right of Figure 26, several learning rates(SGD) **between 0.0002 and 0.002 achieve similar levels of validation accuracy on the original network and maintain that performance to similar levels as the network is pruned**. Of those learning rates, 0.0012 and 0.002 produce the fastest early-stopping times and maintain them to the smallest network sizes

![appendixG2LR](imgs/lottery/appendixG2LR.png)

### Result

1. When randomly reinitialized, a winning ticket
learns more slowly and achieves lower test accuracy, suggesting that initialization is important to
its success

> Random Initæœƒä½¿çš„Winning Ticketå­¸ç¿’é€Ÿåº¦è¼ƒæ…¢ä¸”å¾—åˆ°è¼ƒå·®çš„Test Accuracyï¼Œå› æ­¤Initializationæ˜¯ç›¸ç•¶é‡è¦çš„

2. Usually, the winning tickets we find are 10-20% (or less) of the size of the original network
 
---

## Rethinking The Value of Network Pruning

[PDF Highlight](./RETHINKING_THE_VALUE_OF_NETWORK_PRUNING.pdf)

### Introduction

**é‡å°Pruning æå‡ºä¸‰å€‹çµè«–**

1. training a large, over-parameterized
model is often not necessary to obtain an efficient final model

> (é‡å°3-Stage Network Pruning Pipeline)è¨“ç·´ä¸€å€‹éåƒæ•¸åŒ–(Over-parameterized)çš„æ¨¡å‹ï¼Œå°æ–¼æ‰¾åˆ°æœ€çµ‚å°è€Œæœ‰æ•ˆçš„æ¨¡å‹ä¸¦ä¸å¿…è¦

2. learned â€œimportantâ€ weights of the large model are typically not useful for the small pruned model

> å°æ–¼å°æ¨¡å‹ä¾†èªªï¼Œå­¸ç¿’å¤§æ¨¡å‹è¨“ç·´å‡ºä¾†é‡è¦çš„Weightä¸¦æ²’æœ‰å¹«åŠ©ï¼Œä¹Ÿå°±æ˜¯èªªï¼Œå‚³çµ±Pruningç¹¼æ‰¿å¤§Modelå†Pruneæœ€å¾ŒFine-Tuneçš„åšæ³•æ²’æœ‰å¿…è¦

3. the pruned architecture itself, rather than a set of inherited â€œimportantâ€
weights, is more crucial to the efficiency in the final model

> Prunedçš„Architectureæ¯”Weighté‡è¦

**å…©å€‹å‡ºäººé æ–™çš„è§€å¯Ÿ**

1. For structured pruning methods with predefined target network architectures, directly training the small target model from random initialization can achieve the same

> ä½¿ç”¨Structured Pruning(Predefine å¥½Pruning Rate)å¾Œå¾—åˆ°çš„Modelï¼ŒRandom Initializeå¾Œè¨“ç·´å¯ä»¥é”åˆ°å’ŒåŸæœ¬å¤§Modelä¸€æ¨£(ç”šè‡³æ›´å¥½)çš„Performance

2. For structured pruning methods with autodiscovered target networks, training the pruned model from scratch can also achieve comparable or even better performance than fine-tuning. This observation shows that for these pruning methods, what matters more may be the obtained architecture, instead of the preserved weights

> Pruningå¾Œå¾—åˆ°çœŸæ­£é‡è¦çš„è³‡è¨Šæ˜¯Architectureï¼Œè€ŒéWeights

**å°çµ**

Pruningç®—æ³•æœ€å¾Œå¾—åˆ°çš„å°æ¨¡å‹å…¶å¯¦é‡é»åœ¨æ–¼Pruningå®Œå¾Œçš„"Network Architecture"ï¼Œå¤§æ¨¡å‹è¨“ç·´å‡ºçš„Weightå…¶å¯¦ä¸¦ä¸é‡è¦ï¼Œåœ¨å¾—åˆ°å°æ¨¡å‹å¾Œç›´æ¥Random Initåœ¨è¨“ç·´ä¹Ÿå¯ä»¥å¾—åˆ°ç›¸åŒç”šè‡³æ›´å¥½çš„Test Accuracyã€‚

### Sructured Pruning & Unstructured Pruning

- Sructured Pruning:
  Predefine p% pruning rate(pruned nodes / all nodes) for each layers

- Unstructured Pruning:
  Prune with different pruning rate(pruned nodes / all nodes) for each layers automatically

![Pruning methods](imgs/rethink/pruning_methods.png)
*Sructured Pruning & Unstructured Pruning*

### EXPERIMENTS ON THE LOTTERY TICKET HYPOTHESIS 

In this section, the authors do some experiments and get different results that "random initialization is enough for the pruned model to achieve competitive performance".

å¹¾å€‹é‡è¦çš„ä¸”å’ŒåŸLottery HypothesisåŸä½œä¸ä¸€æ¨£çš„evaluation settings:

1. æ¯”è¼ƒäº†Strucred Pruningçš„çµæœï¼Œè€Œåœ¨Lottery Hypothesisçš„Paperä¸­ï¼Œä½œè€…åªæœ‰æ¯”è¼ƒUnstrucred Pruningçš„çµæœ

2. ä½¿ç”¨è¼ƒModernä¸”è¼ƒå¤§çš„NNï¼ŒLottery Hypothesisçš„åŸä½œè€…ä½¿ç”¨è¼ƒæ·ºçš„NN(Layeræ•¸ < 6)
   
3. ä½¿ç”¨Momentum SGDå’Œè¼ƒå¤§çš„Initial LR(Learning Rate = 0.1, å¸¸ç”¨åœ¨Image Classification)
   
4. ä½¿ç”¨è¼ƒå¤§çš„Dataset(ImageNet Dataset)ï¼ŒLottery HypothesisåŸä½œåªç”¨MNISTå’ŒCIFAR


**Result:**

1. åœ¨éçµæ§‹åŒ–å‰ªæ(Unstructured Pruning)ä¸­ï¼ŒLR(Learning Rate)è¼ƒå¤§çš„æ™‚å€™(0.1)ï¼ŒLottery Ticketç›¸æ¯”æ–¼Random Initæ²’æœ‰å¤ªå¤šå„ªå‹¢ã€‚å¦‚æœLR(Learning Rate)è¼ƒå°(0.1)ï¼Œçµæœèª å¦‚Lottery Hypothesisæ‰€è¨€ï¼ŒLottery Ticket ç¢ºå¯¦æ¯”Random Initå¥½ã€‚ä½†æ˜¯åœ¨å°LRçš„ç‹€æ³ä¸‹ï¼Œç„¡è«–æ˜¯Lottery Ticketå’ŒRandom Initçµæœéƒ½æ¯”å¤§LRçš„çµæœå·®

![Compare to lottery ticket in unstructured pruning1](imgs/rethink/unstructured_prune_res.png)
*LR(Learning Rate)è¼ƒå¤§çš„æ™‚å€™(0.1)ï¼ŒLottery Ticketç›¸æ¯”æ–¼Random Initæ²’æœ‰å¤ªå¤šå„ªå‹¢ï¼›åä¹‹å¦‚æœLR(Learning Rate)è¼ƒå°(0.1)ï¼ŒLottery Ticket ç¢ºå¯¦æ¯”Random Initå¥½*

![Compare to lottery ticket in unstructured pruning1](imgs/rethink/unstructured_prune_res2.png)
*å°LRä½¿Lottery Ticketæ¯”å¤§LRçš„å…©è€…(Lottery Ticket, Random Init)éƒ½æ›´å·®*
   
2. è€Œåœ¨çµæ§‹åŒ–å‰ªæ(Structured Pruning)ä¸­ï¼ŒLottery Ticketä¸¦ä¸æœƒå¸¶ä¾†æ¯”Random Initæ›´å¥½çš„Test Accuracy

![Compare to lottery ticket in structured pruning2](imgs/rethink/structured_prune_res.png)


å¼•è¿°Paperçµè«–

To summarize, in our evaluated settings, the winning ticket only brings improvement in the case of unstructured pruning, with small initial learning rate, but this small learning rate yields inferior accuracy compared with the widely-used large learning rate.

---

## Dawing Early-Bird Tickets More Efficient Training of Deep Networks

[PDF Highlight](./DRAWING_EARLY-BIRD_TICKETS_TOWARDS_MORE_EFFICIENT_TRAINING_OF_DEEP_NETWORKS.pdf)

### Introduction

Discover the Early-Bird (EB) tickets phenomenon: the winning tickets can be drawn very early in training(6.25%, 12.5% in experiments), and with aggressively low-cost training algorithms(5.8 ~ 10.7* energy saving)

### Early-Bird (EB) Tickets Hyppothesis

Consider a dense, randomly-initialized network f(x; Î¸), f reaches a minimum validation loss floss at the i-th iteration with a test accuracy facc, when optimized with SGD on a training set. 

In addition, consider subnetworks f(x; m âŠ™ Î¸) with a mask m âˆˆ {0, 1} indicates the pruned and unpruned connections in f(x; Î¸). When being optimized with SGD on the same training set, f(x; m âŠ™ Î¸) reach a minimum validation loss fâ€² loss at the iâ€²-th iteration with a test accuracy fâ€²acc. 

The EB tickets hypothesis articulates that there **exists m
such that fâ€²acc â‰ˆ facc (even â‰¥), i.e., same or better generalization, with iâ€² â‰ª i (e.g., early stopping)
and sparse m (i.e., much reduced parameters).**

> å­˜åœ¨ä¸€å€‹sparseçš„mask m = {0, 1}ï¼Œä½¿å¾—EB Ticket subnetwork m' = m âŠ™ Î¸ï¼Œå¯ä»¥åœ¨è¨“ç·´åˆ°ç¬¬i'-th iterationæ™‚ï¼Œå°±é”åˆ°f' accuracyï¼Œè€Œi' << i ä¸” f' >= fï¼Œç•¶åŸnetwork Î¸ åœ¨ç¬¬i-th iterationé”åˆ°f accuracy

### Hypothesis Validation

1. Do EB Ticket Always Exist?
   
   p is pruning rate. Sometime over pruning(70% on PreNet101) will make drawing ticket harder

   - EB tickets always emerge at very early stage

   - Some EB tickets are outperform than unpruned full-trained model

   ![ebExist](imgs/eb/ebExist.png)
   *EB Ticket training epochs & Retrain Accuracy*

2. Do EB Tickets Still Emerge under Low-Cost Training?
   
   The meaning of [80, 120] is starting from 0.1, decay to 0.01 at 80-th epoch and further decay to 0.001 at 120 epoch
   
   - Appropriate **Large Learning Rate** is important for emerging EB tickets. The EB tickets always emerge at larger learning rate whose retrain accuray is also better

    ![emergeLowCost](imgs/eb/lowCostLR.png)
    *Learning Rate Schedule & Retrain Accuracy*
  
   - **Low-Precision Training** Dost Not Destroy EB Tickets
     Train & prune the original model with only 8 bits floating point(for all modle weights, activations, gradients and errors). The EB ticket still emerge at very early stage. Then they retrain the EB ticket with full precision(32 bits floating point). It aggressively save energy.

    ![lowPrecision](imgs/eb/lowPrecision.png)
    *Low-Precision Pruning & Retrain Accuracy*
   
### Implement EB Ticket Algorithm

The difference between EB tickets algo and progressie training is only pruned once with pruning ratio *p%* in very early stage(early stopping) and then the EB ticket model will be retrained. The progressive training will train, prune, and retrain the model iterative until the model size reach the target pruning ratio *p%*. What's more? The EB ticket search is much shorter than one progressive training iteration.

NEB â‰ª N (e.g., NEB/N = 12.5% in the experiments
summarized in Figure1 and NEB/N = 6.25% in the experiment summarized in Table 1)

![algoFlow](imgs/eb/algoFlow.png)
*EB Ticket Algorithm Flow Chart*

Here is the pseudo code. The **"mask distance"** will be explained later. Simply to say, the algorithm will train the original network with SGD and **prune the channels in *p%* virtually** to get the mask until **the mask distance < *Æ* which means the mask become stable.**

> The prune ratio *p* is given. 

> The author set the threshold *Æ* as 0.1 (with normalized mask distance of [0, 1]) and the length of queue *l* as 5

> The author doesn't explain scaling factor *r* very clearly. The scaling factor *r* in batch normalization(BN) is a coefficient that mutiply with the weights of the network and it would be trained with SGD. The scaling factor *r* can increase the magnitude of weight and make it easier to identify the neurons should be pruned. It also be uesd in the paper [*Learning Efficient Convolutional Networks through Network Slimming (Liu et al. 2017)*](https://arxiv.org/abs/1708.06519)

![pesudo](imgs/eb/pseudo.png)
*EB Ticket Algorithm Pseudo Code*

![slimmingShot](imgs/eb/slimmingShot.png)
*The paragraph that mention about the scalling factor r in "Learning Efficient Convolutional Networks through Network Slimming"(Liu et al. 2017)*

The mask is a matrix that determine whether the channel should be pruned or not. The mask is binary that will denote the pruned channels as 0 while keep ones as 1. The **mask distance** represent the **difference** between 2 ticket masks in [**Hamming distance**](https://en.wikipedia.org/wiki/Hamming_distance).

For the following figure, it shows the mask distance drawn from different epoch where **(i, j)-th element in the matrix denotes the mask distance in normalized(by the size of original network) between subnetwork drawn from i-th and j-th epochs.** The warmer, the closer. The red lines denote 0.1 in mask distance. It shows the masks of EB tickets are determined in a very early stage.

![maskDis](imgs/eb/maskDis.png)
*Pairwise Mask Distance in Hamming Distance*

### Experiments



### Conclusion

## Learning Efficient Convolutional Networks through Network Slimming

[PDF Highlight](./Learning_Efficient_Convolutional_Networks_through_Network_Slimming.pdf)

## Weight Agnostic Neural Networks

[PDF Highlight](./Weight_Agnostic_Neural_Networks.pdf)

### Initialization and Regularization of Factorized Neural Layers
[PDF Highlight](./Initialization%20and%20Regularization%20of%20Factorized%20Neural%20Layers.pdf)