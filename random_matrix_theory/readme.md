
# Random Matrix Theory Proves that Deep Learning Representations of GAN-data Behave as Gaussian Mixtures

Mohamed El Amine Seddik, Cosme Louart,Mohamed Tamaazousti, Romain Couillet

CEA List, Centralesup´elec, GIPSA-Lab

ICML'20

Citation: 28

# Applicability of Random Matrix Theory in Deep Learning

Nicholas P. Baskerville, Diego Granziol, Jonathan P. Keating

University of Bristol, University of Oxford

# Nonlinear random matrix theory for deep learning

Jeffrey Pennington, Pratik Worah

Google Brain

NIPS'17

Citation: 110

# Geometry of Neural Network Loss Surfaces via Random Matrix Theory

Jeffrey Pennington, Yasaman Bahri 

Google Brain

ICML'17

Citation: 112

# Spectra of the Conjugate Kernel and Neural Tangent Kernel for Linear-Width Neural Networks

Zhou Fan, Zhichao Wang

Yale University, University of California, San Diego

NIPS'20

Citation: 22

# On the Random Conjugate Kernel and Neural Tangent Kernel

Zhengmian Hu, Heng Huang 

University of Pittsburgh

ICML'21

Citation: ?

# Reference

- [ICML'21 Random Matrix Theory and ML (RMT+ML)](https://icml.cc/Conferences/2021/Schedule?showEvent=10840)
- [知乎 随机矩阵不随机](https://zhuanlan.zhihu.com/RandomMatrixTheory)
- [Yue Lu | Nov 30, 2021 | Learning by Random Features and Kernel Random Matrices](https://www.youtube.com/watch?v=RN6nFNbSGE8)
  - Mention that for any non-linear activation function can be approximated by a Gaussian noise model, called **Gaussian Equivalence Theorem**
- [Romain Couillet -- Mini course: Why Random Matrices can Change the Future of Research in AI?](https://www.youtube.com/watch?v=jccL2q3EAOo)