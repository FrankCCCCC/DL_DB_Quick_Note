# DL & DB Quick Notes
A collection of papers for DL and DB researchers

## Fields
- RL
  - [Algorithms](RL/rl_algos)
  - [Intrisic Curiosity](RL/curiosity)
  - [Curricular Learning with RL](RL/curricular_learning_with_rl)
  - [Distributed Training](RL/distributed)
  - [Open-Ended RL](RL/open_ended)
  - [World-Model Based RL](RL/world_model)
  - [RL & Biology](RL/biology)
  - [RL Applications](RL/applications)
  
- Lottery Hypothesis & Pruning
  
  Here is the [Notes](lottery_hypothesis_and_pruning/readme.md) of papers

  - [Proving the Lottery Ticket Hypothesis Pruning is All You Need](lottery_hypothesis_and_pruning/Proving_the_Lottery_Ticket_Hypothesis_Pruning_is_All_You_Need.pdf)
  - [The Lottery Ticket Hypothesis Finding Sparse Trainable Neural Networks](lottery_hypothesis_and_pruning/THE_LOTTERY_TICKET_HYPOTHESIS_FINDING_SPARSE_TRAINABLE_NEURAL_NETWORKS.pdf)
  - [Rethinking The Value of Network Pruning](lottery_hypothesis_and_pruning/RETHINKING_THE_VALUE_OF_NETWORK_PRUNING.pdf)
  - [Dawing Early-Bird Tickets More Efficient Training of Deep Networks](lottery_hypothesis_and_pruning/DRAWING_EARLY-BIRD_TICKETS_TOWARDS_MORE_EFFICIENT_TRAINING_OF_DEEP_NETWORKS.pdf)
  - [Weight Agnostic Neural Networks](lottery_hypothesis_and_pruning/Weight_Agnostic_Neural_Networks.pdf)
  - [Learning Efficient Convolutional Networks through Network Slimming](lottery_hypothesis_and_pruning/Learning_Efficient_Convolutional_Networks_through_Network_Slimming.pdf)
- Neural Tangent Kerel(NTK)
  
  [Notes](./ntk/readme.md)

  - [Wide Neural Networks of Any Depth Evolve as Linear Models Under Gradient Descent](ntk/Wide%20Neural%20Networks%20of%20Any%20Depth%20Evolve%20as.pdf)
  - [DEEP INFORMATION PROPAGATION](ntk/deep_information_propagation.pdf)
  - [The large learning rate phase of deep learning: the catapult mechanism](ntk/The%20large%20learning%20rate%20phase%20of%20deep%20learning.pdf)
- Transformer
  
  Here is the [Notes](./transformer/readme.md) of papers
  - [COMPRESSIVE TRANSFORMERS FOR LONG-RANGE SEQUENCE MODELLING](./transformer/Compressive%20Transformers%20For%20Long-Range%20Sequence%20Model.pdf)
  - [Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context](./transformer/Transformer-XL%20Attentive%20Language%20Models%20Beyond%20a%20Fixed-Length%20Context.pdf)
- Meta-Learning
- BERT
- DL & Cyber Security
- DL Applications